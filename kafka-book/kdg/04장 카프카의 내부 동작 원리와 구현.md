# 카프카 리플리케이션
<hr>

#### 토픽 생성

- 아래 명령어를 사용하여 파티션 3개, 리플리케이션 팩터 3개의 토픽을 생성합니다.
```
kafka-topics --bootstrap-server localhost:9092 --create --topic topic-p3-r3 --partitions 3 --replication-factor 3
```

#### 토픽 조회

- 토팍을 조회하면 정상적으로 만들어진 것을 알 수 있고, 파티션이 복제된것도 확인할 수 있습니다.

```
kafka-topics --bootstrap-server localhost:9092 --describe --topic topic-p3-r3
```

![스크린샷 2024-02-01 오후 10 53 29](https://github.com/kdg0209/realizers/assets/80187200/8156a4be-e70d-4c76-be93-df6e52040318)

#### 그림으로 이해하기

![스크린샷 2024-02-01 오후 10 57 52](https://github.com/kdg0209/realizers/assets/80187200/daf478b7-9bf6-4f3a-b24c-c4f0296394e4)

<br>

## Leader Partition

- Producer와 Consumer는 Leader Partition을 통해 읽기와 쓰기를 수행합니다.
- 즉 Producer는 메시지 전송시 모든 리플리케이션에 메시지를 전송하는게 아니라 Leader Partition에게만 메시지를 전송합니다. 
또한 Consumer도 Leader Partition으로부터 메시지를 가져옵니다.

#### 💡 Kafka 2.4 업데이트

- kafka 2.4 버전부터 Consumer는 Follower Partition으로부터 메시지를 가져올 수 있습니다.(Follower Fetching)
- https://www.conduktor.io/kafka/kafka-topic-replication/#acks-=-all-2
- https://developers.redhat.com/blog/2020/04/29/consuming-messages-from-closest-replicas-in-apache-kafka-2-4-0-and-amq-streams#

<br>

## Leader와 Follower의 리플리케이션 동작

- 카프카는 리플리케이션 과정에서 ACK 통신 단계를 제거함으로써 성능을 높였습니다.
- 우선 리플리케이션의 동작을 원리를 파악하기 위해서 다음과 같은 설정을 해야합니다.
- acks: all
- min.insync.replicas: 3 (최소 리플리케이션 수)

#### 과정

1. Leader Partition만이 0번 오프셋에 초록색 새로운 메시지를 가지고 있습니다.
2. Follower Partition들은 Leader에게 0번 오프셋의 메시지를 가져오기 위해 Fetch 요청을 보낸 후 새로운 메시지(초록색)가 있다는 사실을 인지하고 리플리케이션하게 됩니다.
3. Leader Partition은 모든 Follower가 0번 오프셋 메시지에 대해 Fetch 요청을 보냈다는 사실을 알고 있습니다.(하지만 Leader는 Follower가 성공적으로 리플리케이션 했는지는 모릅니다.)

![스크린샷 2024-02-01 오후 11 49 17](https://github.com/kdg0209/realizers/assets/80187200/247270aa-f086-4fb4-a7d9-a10d34db14bf)

4. 3번째 flow에서 Producer로부터 새로운 메시지를 받게됩니다.
5. 0번 오프셋에 대한 리플리케이션을 마친 Follower들은 Leader에게 다음 오프셋을 Fetch하게 됩니다.
6. Follower들로부터 1번 오프셋에 대한 Fetch 요청을 받은 Leader는 0번 오프셋이 정상적으로 복제되었음을 인지하고 오프셋 0번을 커밋하게 됩니다.
   - 만약 Follower가 0번 오프셋을 정상적으로 복제하지 못한다면 Follower는 다시 Fetch 요청을 했을 때 1번 오프셋이 아닌 0번 오프셋에 대해 Fetch 요청을 보내개 됩니다. 따라서 Leader는 Follower가 요청한 오프셋을 보고 성공했는지 인지할 수 있습니다.
7. Follower들로부터 1번 오프셋 Fetch를 받은 Leader는 응답에 0번 오프셋 메시지가 커밋되었다는 내용도 함께 전달합니다. 그리고 Leader의 응답을 받은 모든 Follower도 0번 오프셋 메시지가 커밋되었다는 것을 알게되고, 커밋을 표시하게 됩니다.

![스크린샷 2024-02-01 오후 11 56 28](https://github.com/kdg0209/realizers/assets/80187200/c5a4102e-4276-4100-baf6-6215d539c937)

<br>

## Isr(In-Sync-Replica)

- Leader와 Follower는 Isr이라는 논리적 그룹으로 묶여 있습니다. 
- 논리적 그룹으로 나누는 이유는 해당 그룹에 속한 Follower들만이 Leader가 죽은 경우 새로운 Leader의 자격을 가질 수 있기 때문입니다. 다시말해 Isr 그룹에 속하지 못한 Follower는 새로운 Reader의 자격이 될 수 없습니다.
- 위 리플리케이션 동작 과정을 통해 Isr내에 있는 모든 Follower에게 복제가 완료되면, Leader는 내부적으로 커밋되었다는 표시를 하게된다는 것을 알게되었습니다. 마지막 오프셋 위치는 하이워터마크라고 부릅니다.
- 즉 커밋되었다는 것은 리플리케이션 팩터 수의 모든 리플리케이션이 전부 메시지를 저장했음을 의미합니다. 그리고 이렇게 커밋된 메시지만 Consumer가 읽어갈 수 있습니다.

<br>

## Leader Epoch와 복구

<br>

## Controller

- kafka 클러스터 중에서 하나의 브로커가 컨트롤러 역할을 수행하게되며, 파티션의 Isr 리스트 중에서 리더 파티션을 선출하게 됩니다.
- Isr 정보는 주키퍼에 저장되게 됩니다.
- 컨트롤러는 브로커의 상태를 예의주시하고 있으며, 만약 동기화가 제대로 이루어지지 않는 브로커는 Isr에서 제외되며 해당 브로커가 가지고 있던 파티션은 다른 브로커에게 리더를 넘겨주게 됩니다.

```
1. 주키퍼 쉘 접속(bin/zookeeper-shell)
zookeeper-shell localhost:2181
```

#### 예상치 않게 브로커가 다운된 경우

1. Broker 3이 다운됨
2. Zookeeper는 45초 뒤에 Broker 3이 다운된걸 알게됨
3. Controller는 Zookeeper를 모니터링하다가 Watch Event로 Broker 3이 다운된걸 알게됨
4. Controller는 다운된 Broker가 관리하던 파티션에 대해 Leader와 Follower를 결정함
5. 결정된 새로운 Leader/Follower는 Zookeeper에 저장하고, 모든 브로커에게 새로운 Leader/Follower 정보를 전달하고 새로운 Leader로부터 복제 수행할것을 요청
6. Controller는 모든 Broker가 가지는 MetadataCache를 새로운 Leader/Follower 정보로 갱신할 것을 요청

<img width="1018" alt="스크린샷 2024-02-03 오후 5 32 13" src="https://github.com/kdg0209/realizers/assets/80187200/de4c6507-ac70-4965-bee2-b07a0e44c80b">

#### SIG_TERM으로 브로커가 다운된 경우

- 과정은 아래와 같습니다.
  
<img width="1024" alt="스크린샷 2024-02-03 오후 5 40 12" src="https://github.com/kdg0209/realizers/assets/80187200/330e1bd3-e0e0-42fe-b1a0-7a7b3731c11c">

#### 🤔 무슨 차이가 있을까?

- 다운 타임과 상관있습니다.
- 제어된 종료를 사용하면 카프카 내부적으로 파티션들의 다운타임을 최소화할 수 있습니다. 그 이유는 컨트롤러가 해당 브로커가 리더로 할당된 파티션에 대해 리더 선출 작업을 하기 때문이라고 합니다. (무슨 말인지 모르겠음)

<br>

## 멀티 브로커 환경에서 테스트 1 (브로커 다 죽이고 컨트롤러 브로커 제외 실행)

- 상황
   - 브로커 A, B, C를 실행 시킨 상태입니다.(여기서 컨트롤러 브러커는 A입니다.)
   - 브로커를 C -> B -> A 순서로 다운을 시킵니다.
   - 브로커 B 또는 C 만을 다시 실행시킵니다. (컨트롤러 브로커 제외)
   - 이때 토픽을 상세조회하면 Leader가 None이 됩니다.
   - 아무리 시간이 지나도 Leader Partition 세팅이 안되어 Producer와 Consumer가 먹통이됩니다.
 
```
1. 테스트 토픽 생성
kafka-topics --bootstrap-server localhost:9092 --create --topic 0204-multi-topic --partitions 3 --replication-factor 3

2. 토픽 조회
kafka-topics --bootstrap-server localhost:9092 --describe --topic 0204-multi-topic

3. java code에서 브로커 A, B, C를 producer 및 consumer를 설정합니다.
```

- 메시지를 잘 보내고 받는것을 확인할 수 있습니다.
  
<img width="983" alt="스크린샷 2024-02-04 오후 4 29 12" src="https://github.com/kdg0209/realizers/assets/80187200/c1d57f9a-7a02-4276-a5fd-62f86a751a2d">

- 이제 브로커를 C -> B -> A 순서로 다운시킵니다. 
- 브로커 B만을 실행시킵니다.
- 시간이 지나도 Leader는 None으로 설정되고, Producer와 Consumer는 동작을 안하게 되어 문제가 발생합니다.

<img width="1024" alt="스크린샷 2024-02-04 오후 4 32 18" src="https://github.com/kdg0209/realizers/assets/80187200/3ee0800d-8300-4b1c-b742-9e54ab2669b9">

### 💡 어떻게 해결할 것인가?

- unclean.leader.election.enable를 true로 설정하여, 빠르게 재 실행된 브로커 중에서 컨트롤러 브로커를 수행할 수 있도록 합니다.(기본값: flase)
- 하지만 이 방법은 메시지 손실이 발생할 수 있습니다.

```
1. 아래 설정으로 토픽 단위로 설정할 수 있습니다.
kafka-configs --bootstrap-server localhost:9092 --entity-type topics --entity-name  0204-multi-topic --alter --add-config unclean.leader.election.enable=true
```

- 다시 테스트를 진행하기 위해 모든 브로커를 죽이고 다시 실행시킵니다.
- 현재 컨트롤러 브로커는 B 이므로 브로커를 C -> A -> B 순서로 다운 시킵니다.(컨트롤러 브로커만 마지막에 다운 시키면됩니다.)
- 브로커 C 또는 A를 실행시킵니다.

<img width="1039" alt="스크린샷 2024-02-04 오후 4 45 45" src="https://github.com/kdg0209/realizers/assets/80187200/29b13bb0-acd8-47c3-a59d-930ce9fff5f7">

<br>

## 멀티 브로커 환경에서 테스트 2 (브로커 다 죽이고 컨트롤러 우선 실행)

```
kafka-topics --bootstrap-server localhost:9092 --create --topic 0204-multi-topic-1 --partitions 3 --replication-factor 3
```

- 상황
   - 브로커 A, B, C를 실행 시킨 상태입니다.(여기서 컨트롤러 브러커는 A입니다.)
   - 브로커를 C -> B -> A 순서로 다운을 시킵니다.
   - 브로커 A -> B -> C 순서로 다시 실행시킵니다.
   - 이때는 잘 되네?!

<br>

## 로그(세그먼트)

- 카프카의 토픽으로 들어오는 레코드(메시지)는 segment라는 파일에 저장됩니다.
- segment에는 key, value, offset, payload와 같은 정보가 브로커의 로컬 디스크에 저장됩니다.
- segment의 기본 크기는 1GB로 설정되어 있습니다. segment의 크기가 1GB보다 커지는 경우 해당 segment는 close되고, 롤링 전략을 적용되게 됩니다.
- close된 segment는 읽기만 가능합니다.

<img width="960" alt="스크린샷 2024-02-03 오후 1 13 28" src="https://github.com/kdg0209/realizers/assets/80187200/47c9a4fb-de48-416b-9af2-62cd6201cb32">

<br>


### rolling 매커니즘 테스트

#### segment.bytes를 10KB로 하여 테스트 진행

```
1. 예제 토픽 생성
kafka-topics --bootstrap-server localhost:9092 --create --topic segment-test-topic --partitions 3

2. 생성한 토픽의 segment byte 수정
kafka-configs --bootstrap-server localhost:9092 --entity-type topics --entity-name segment-test-topic --alter --add-config segment.bytes=10240

3. 변경한 내용 확인
kafka-configs --bootstrap-server localhost:9092 --entity-type topics --entity-name segment-test-topic --all --describe | grep segment.bytes

4. 아래와 같이 출력되면 정상적으로 변경된 것
segment.bytes=10240 sensitive=false synonyms={DYNAMIC_TOPIC_CONFIG:segment.bytes=10240, STATIC_BROKER_CONFIG:log.segment.bytes=1073741824, DEFAULT_CONFIG:log.segment.bytes=1073741824}

5. 테스트 메시지 파일 작성
for i in {1..2000}
do
echo "test message send test-i" >> message.log
done

6. 파일 기반으로 메시지 전송
kafka-console-producer --bootstrap-server localhost:9092 --topic segment-test-topic < message.log
```

<img width="1038" alt="스크린샷 2024-02-03 오후 1 46 50" src="https://github.com/kdg0209/realizers/assets/80187200/5cbdbfac-0c84-4888-b5a3-d3cba6a69c2b">

#### segment.ms를 60초 하여 테스트 진행

```
1. segment.byte를 기본 속성값으로 되돌림
kafka-configs --bootstrap-server localhost:9092 --entity-type topics --entity-name segment-test-topic --alter --delete-config segment.bytes

2. segment.ms를 60초로 수정
kafka-configs --bootstrap-server localhost:9092 --entity-type topics --entity-name segment-test-topic --alter --add-config segment.ms=60000

3. 파일 기반으로 메시지 전송
kafka-console-producer --bootstrap-server localhost:9092 --topic segment-test-topic < message.log
```

<img width="997" alt="스크린샷 2024-02-03 오후 2 02 38" src="https://github.com/kdg0209/realizers/assets/80187200/2ed997f7-fd56-46ec-b186-3268ad2fb62a">


#### 🤔 index, timeindex는 무엇일까?

- 우선 log.index.interval.bytes를 알아야 하는데 해당 속성은 새로운 항목이 offset index에 추가되는 주기이다.
- index는 모든 offset에 대한 byte position을 가지고 있지 않으며, log.index.interval.bytes에 설정된 만큼의 segment byte가 만들어질때마다 해당 offeset에 대한 byte position 정보를 가지고 있습니다.
- timeindex는 메시지의 생성 Unix 시간을 ms 단위로 가지고 있고, 해당 생성 시간에 해당하는 offset정보를 가지고 있습니다.
- 따라서 index, timeindex를 가지고 random access시 파일의 용량만큼 수행 시간(scan 시간)이 소요되는게 아니라 조금 더 효율적으로 찾을 수 있게 됩니다.

```
1. log.index.interval.bytes 확인 (기본 4KB)
kafka-configs --bootstrap-server localhost:9092 --entity-type topics --entity-name segment-test-topic --all --describe | grep log.index.interval.bytes
```

<img width="1041" alt="스크린샷 2024-02-03 오후 2 31 49" src="https://github.com/kdg0209/realizers/assets/80187200/35b0cd81-5a85-47a9-ae0f-9aa953945285">

<br>

### Log Cleanup Policy 

#### delete

- delete로 설정하면 segment를 log.retention.hours나 log.retention.bytes 설정값에 따라 삭제합니다.

#### compact

- compact로 설정하면 segment를 key 값 레벨로 가장 최신의 메시지만 유지하도록 segment를 재구성 합니다.

#### delete, compact

- compact와 delete를 함께 적용합니다.

#### delete 예제

- log.retention.hours: 개별 segment의 유지 시간(기본 7일)
- log.retention.bytes: 삭제하기 전에 보관할 파티션 당 로그 수. 기본값은 -1. 토픽별로 설정이 가능하며, 로그의 시간이나 크기제한에 도달하면 삭제됩니다.
- log.retention.check.interval.ms: 브로커가 백그라운드로 삭제할 segment를 찾는 주기입니다.

```
1. 기본 삭제 정책 조회(기본: delete)
kafka-configs --bootstrap-server localhost:9092 --entity-type topics --entity-name segment-test-topic --all --describe | grep log.cleanup.policy

2. 브로커의 segment 유지 시간 조회(기본: 7일)
kafka-configs --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --all --describe | grep log.retention.hours

3. 토픽의 retention 확인(기본: retention.ms=604800000(7일))
kafka-configs --bootstrap-server localhost:9092 --entity-type topics --entity-name segment-test-topic --all --describe | grep retention

4. 테스트를 위해 retention.bytes를 10KB로 수정
kafka-configs --bootstrap-server localhost:9092 --entity-type topics --entity-name segment-test-topic --alter --add-config retention.bytes=10240

5. 테스트를 위해 retention.ms를 3분으로 수정
kafka-configs --bootstrap-server localhost:9092 --entity-type topics --entity-name segment-test-topic --alter --add-config retention.ms=180000
```

#### 💡 참고

- Active Segment지만 아무런 메시지가 안온 상태에서 log.retention.hours의 시간이 지나면 삭제 대상이 될 수 있습니다.

<img width="1026" alt="스크린샷 2024-02-03 오후 3 05 52" src="https://github.com/kdg0209/realizers/assets/80187200/7670d3bc-9c68-49cf-b439-b823ededc60c">

#### compact

- log.cleanup.policy를 compact로 설정시 segment의 key 값에 따라 가장 최신의 메시지만 segment를 재구성하게 됩니다.
- key가 null인 메시지는 적용할 수 없습니다.
- log cleaner가 백그라운드 스레드 방식으로 별도의 I/O 작업을 수행하므로 추가적인 I/O 부하가 소모됩니다.
- log compaction을 사용하면 빠른 장애 복구를 할수 있습니다. 장애 복구 시 전체 로그를 복구하지 않고, 메시지의 키를 기준으로 최신 상태만 복구하며, 따라서 전체 로그를 복구할 때보다 복구 시간을 줄일 수 있습니다.

<img width="983" alt="스크린샷 2024-02-03 오후 3 31 22" src="https://github.com/kdg0209/realizers/assets/80187200/ae2a8520-69f8-4275-aa2c-12d817b87e1f">

#### 예제

```
1. 예제 토픽 생성
kafka-topics --bootstrap-server localhost:9092 --create --topic compact-test-topic --partitions 3

2. 테스트를 위해 retention.bytes를 10KB로 수정
kafka-configs --bootstrap-server localhost:9092 --entity-type topics --entity-name compact-test-topic --alter --add-config retention.bytes=10240

3. 테스트를 위해 cleanup.policy를 compact로 수정
kafka-configs --bootstrap-server localhost:9092 --entity-type topics --entity-name compact-test-topic --alter --add-config cleanup.policy=compact

4. log compaction 모니터링
- compact-test-topic의 log 디렉토리에서 수행
while true
do
sleep 10
echo "######## `date` ########"
ls -li
done

5. key 메시지 2000개를 keyload.log에 기록하기. 
for i in {1..2000}
do
echo "$i:test message send i" >> key-message.log
done

6. 파일 기반으로 메시지 전송
kafka-console-producer --bootstrap-server localhost:9092 --topic compact-test-topic < key-message.log --property key.separator=: --property parse.key=true
```








